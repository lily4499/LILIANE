üî• Alert via Slack When CPU Usage is Too High (Prometheus + Alertmanager)

To monitor high CPU usage in your Kubernetes cluster and send alerts to Slack, we will:

‚úÖ Use Prometheus to track CPU usage
‚úÖ Configure Alertmanager to send alerts
‚úÖ Integrate Slack notifications
1Ô∏è‚É£ Install Prometheus & Alertmanager

If Prometheus is not yet installed, install it using:

kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/bundle.yaml

Verify:

kubectl get pods -n monitoring

If Prometheus is running, proceed.
2Ô∏è‚É£ Create a Slack Webhook

    Go to Slack API:
    üëâ https://api.slack.com/apps
    Create a New App ‚Üí Incoming Webhooks ‚Üí Add Webhook.
    Select a Slack Channel (e.g., #alerts).
    Copy the Webhook URL, which looks like:

    https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX

3Ô∏è‚É£ Configure Alertmanager to Send Alerts to Slack

Create a Kubernetes Secret to store the Slack webhook.
alertmanager-slack-secret.yaml

apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-slack-secret
  namespace: monitoring
type: Opaque
data:
  slack-webhook-url: <BASE64_ENCODED_WEBHOOK>

To encode the webhook, run:

echo -n "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX" | base64

Replace <BASE64_ENCODED_WEBHOOK> in the YAML file with the output.

Apply:

kubectl apply -f alertmanager-slack-secret.yaml

4Ô∏è‚É£ Create an Alert Rule for High CPU Usage

Prometheus needs rules to detect high CPU usage.
cpu-alert-rules.yaml

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: high-cpu-alert
  namespace: monitoring
spec:
  groups:
  - name: cpu-alerts
    rules:
    - alert: HighCPUUsage
      expr: (100 * sum(rate(container_cpu_usage_seconds_total{container!="",namespace!="kube-system"}[5m])) / sum(machine_cpu_cores)) > 80
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage has exceeded 80% for more than 2 minutes."

Apply:

kubectl apply -f cpu-alert-rules.yaml

5Ô∏è‚É£ Configure Alertmanager to Send Alerts

Create an Alertmanager configuration that reads the Slack webhook.
alertmanager-config.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
    route:
      receiver: "slack-alerts"
      repeat_interval: 30m
    receivers:
    - name: "slack-alerts"
      slack_configs:
      - channel: "#alerts"
        send_resolved: true
        username: "Prometheus"
        icon_emoji: ":warning:"
        api_url: "${SLACK_WEBHOOK_URL}"

Apply:

kubectl apply -f alertmanager-config.yaml

6Ô∏è‚É£ Deploy Alertmanager with Slack Integration

Create the Alertmanager deployment.
alertmanager-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager
        args:
          - "--config.file=/etc/alertmanager/alertmanager.yml"
        env:
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: alertmanager-slack-secret
              key: slack-webhook-url
        volumeMounts:
          - name: config-volume
            mountPath: /etc/alertmanager
      volumes:
      - name: config-volume
        configMap:
          name: alertmanager-config
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager-service
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
  - protocol: TCP
    port: 9093
    targetPort: 9093

Apply:

kubectl apply -f alertmanager-deployment.yaml

üöÄ Final Steps

    Restart Prometheus to load new rules:

kubectl delete pod -n monitoring -l app=prometheus

Test the alert by generating CPU load:

kubectl run cpu-load --image=busybox --restart=Never -- sh -c "while true; do :; done"

This simulates high CPU usage.
Wait 2 minutes, then check Slack for an alert.
Delete the load test pod:

    kubectl delete pod cpu-load

‚úÖ Summary

‚úÖ Alert triggers when CPU exceeds 80% for 2 minutes.
‚úÖ Slack notifies the #alerts channel.
‚úÖ Alertmanager handles alert delivery.

Would you like to:

    Add memory usage alerts?
    Auto-scale pods when CPU is high?
